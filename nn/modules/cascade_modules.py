import torch

import torch.nn as nn
import torch.nn.functional as F
from .block import C2f, Bottleneck
import math
import ultralytics
print(f"ğŸ”¥ å½“å‰åŠ è½½çš„åº“è·¯å¾„: {ultralytics.__file__}")
# ==========================================
# åˆ›æ–°ç‚¹ä¸€ï¼šå¯å¾®æ³¨è§†å˜æ¢æ¨¡å— (Differentiable Gaze Shift)
# ==========================================
# class DifferentiableGazeShift(nn.Module):
#     def __init__(self, out_size=(640, 640)):
#         """
#         args:
#             out_size: ç»†è§†ç½‘ç»œéœ€è¦çš„è¾“å…¥å°ºå¯¸ (H, W)
#         """
#         super().__init__()
#         self.out_size = out_size

#     def forward(self, x, crop_params):
#         """
#         x: è¾“å…¥çš„å…¨å›¾ Tensor [B, C, H_in, W_in]
#         crop_params: ç²—æ£€æµ‹å™¨è¾“å‡ºçš„è£å‰ªå‚æ•° [B, 3] -> (tx, ty, scale)
#                      tx, ty èŒƒå›´åœ¨ [-1, 1], scale èŒƒå›´ (0, 1]
#         """
#         B, C, H, W = x.shape
        
#         # 1. æ„å»ºä»¿å°„å˜æ¢çŸ©é˜µ theta [B, 2, 3]
#         #    [ sx, 0, tx ]
#         #    [ 0, sy, ty ]
#         theta = torch.zeros(B, 2, 3, device=x.device, dtype=x.dtype)
        
#         # ç¼©æ”¾å› å­ (scale)ã€‚æ³¨æ„ï¼šSTNä¸­ scaleè¶Šå°ï¼Œè§†é‡è¶Šå°(æ”¾å¤§å€æ•°è¶Šå¤§)
#         # è¿™é‡Œå‡è®¾ä¼ å…¥çš„ scale æ˜¯ "ä¿ç•™åŒºåŸŸçš„æ¯”ä¾‹"ï¼Œä¾‹å¦‚ 0.5 ä»£è¡¨å–ä¸€åŠé•¿å®½
#         s = crop_params[:, 2] 
#         tx = crop_params[:, 0]
#         ty = crop_params[:, 1]

#         theta[:, 0, 0] = s
#         theta[:, 1, 1] = s
#         theta[:, 0, 2] = tx
#         theta[:, 1, 2] = ty

#         # 2. ç”Ÿæˆé‡‡æ ·ç½‘æ ¼ (Affine Grid)
#         # æ³¨æ„ï¼šsize éœ€è¦æ˜¯ (B, C, H_out, W_out)
#         grid = F.affine_grid(theta, torch.Size((B, C, self.out_size[0], self.out_size[1])), align_corners=False)

#         # 3. å¯å¾®é‡‡æ · (Differentiable Sampling / Bilinear Interpolation)
#         # è¿™å°±æ˜¯å…¬å¼ V_out(x,y) çš„ä»£ç å®ç°
#         x_cropped = F.grid_sample(x, grid, mode='bilinear', padding_mode='zeros', align_corners=False)

#         return x_cropped


class DifferentiableGazeShift(nn.Module):
    def __init__(self, out_size=(160, 160)):
        super().__init__()
        self.out_size = out_size

    def forward(self, x, crop_params):
        if isinstance(x, list): x = x[0]
        B, C, H, W = x.shape
        
        # crop_params: [B, 3] -> (tx, ty, s)
        tx = crop_params[:, 0]
        ty = crop_params[:, 1]
        s = crop_params[:, 2]
        
        # æ„å»º Affine Matrix [B, 2, 3]
        theta = torch.zeros(B, 2, 3, device=x.device, dtype=x.dtype)
        
        # s æ§åˆ¶ç¼©æ”¾: s=1 (å…¨å›¾), s=0.5 (æ”¾å¤§)
        # å¯¹åº”çŸ©é˜µå¯¹è§’çº¿å…ƒç´ 
        theta[:, 0, 0] = s
        theta[:, 1, 1] = s
        
        # tx, ty æ§åˆ¶å¹³ç§»
        # åœ¨ affine_grid ä¸­ï¼ŒT æ˜¯åŠ åœ¨ (s*x) ä¸Šçš„
        # æˆ‘ä»¬å·²ç»æŠŠ tx, ty å¤„ç†æˆäº† grid_sample éœ€è¦çš„ä¸­å¿ƒåæ ‡ [-1, 1]
        # ä½† affine_grid çš„å…¬å¼æ˜¯ x_in = theta * x_out
        # x_out èŒƒå›´æ˜¯ -1..1
        # å½“ x_out=0 (ä¸­å¿ƒ) æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ› x_in = tx
        # æ‰€ä»¥ theta[:, 0, 2] åº”è¯¥ç›´æ¥ç­‰äº tx
        theta[:, 0, 2] = tx
        theta[:, 1, 2] = ty

        # ç”Ÿæˆç½‘æ ¼
        grid = F.affine_grid(theta, torch.Size((B, C, self.out_size[0], self.out_size[1])), align_corners=False)
        
        # é‡‡æ · (padding_mode='zeros' ä¼šäº§ç”Ÿé»‘è¾¹ï¼Œä½†å› ä¸ºæˆ‘ä»¬åœ¨ tasks.py åšäº† clampï¼Œè¿™é‡Œåº”è¯¥ä¸ä¼šè§¦å‘äº†)
        x_cropped = F.grid_sample(x, grid, mode='bilinear', padding_mode='zeros', align_corners=False)

        return x_cropped

# ==========================================
# åˆ›æ–°ç‚¹äºŒï¼šå…¨å±€-å±€éƒ¨ä¸Šä¸‹æ–‡çº ç¼ æ¨¡å— (GL-Context Entanglement)
# ==========================================
# class GL_ContextBlock(nn.Module):
#     def __init__(self, c_local, c_global, c_out, nhead=4):
#         """
#         c_local: ç»†è§†ç‰¹å¾é€šé“æ•° (Query)
#         c_global: ç²—è§†ç‰¹å¾é€šé“æ•° (Key/Value)
#         c_out: è¾“å‡ºé€šé“æ•°
#         """
#         super().__init__()
#         self.norm_local = nn.LayerNorm(c_local)
#         self.norm_global = nn.LayerNorm(c_global)
        
#         # è¿™é‡Œçš„ Cross Attention å¯ä»¥ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ï¼Œä¹Ÿå¯ä»¥æ‰‹å†™ä»¥æ›´å¥½åœ°æ§åˆ¶
#         # ä¸ºäº†æ–¹ä¾¿é›†æˆï¼Œæˆ‘ä»¬ä½¿ç”¨ nn.MultiheadAttention
#         # æ³¨æ„ï¼šTransformeré€šå¸¸ä¸»è¦åœ¨ dim ç»´åº¦æ“ä½œï¼ŒConvå±‚éœ€è¦ permute
#         self.cross_attn = nn.MultiheadAttention(embed_dim=c_local, kdim=c_global, vdim=c_global, num_heads=nhead, batch_first=True)
        
#         self.proj = nn.Conv2d(c_local, c_out, 1) if c_local != c_out else nn.Identity()

#     def forward(self, x_local, x_global):
#         """
#         x_local: [B, C_l, H_l, W_l] (Fine Feature)
#         x_global: [B, C_g, H_g, W_g] (Coarse Feature)
#         """
#         B, C_l, H_l, W_l = x_local.shape
#         B, C_g, H_g, W_g = x_global.shape

#         # 1. å¯¹é½ç©ºé—´ç‰¹å¾ (Flatten)
#         # [B, H_l*W_l, C_l]
#         q = x_local.flatten(2).permute(0, 2, 1)
#         # [B, H_g*W_g, C_g]
#         k = x_global.flatten(2).permute(0, 2, 1)
#         v = x_global.flatten(2).permute(0, 2, 1)

#         # å½’ä¸€åŒ– (LayerNorm)
#         q = self.norm_local(q)
#         k = self.norm_global(k)
#         v = self.norm_global(v)

#         # 2. Cross Attention: Query=Local, Key/Value=Global
#         # å…¬å¼: Softmax(Q * K.T / sqrt(d)) * V
#         attn_out, _ = self.cross_attn(query=q, key=k, value=v)

#         # 3. æ®‹å·®è¿æ¥ + å½¢çŠ¶è¿˜åŸ
#         # Fused = LayerNorm(Q + Attention) ... è¿™é‡Œç®€å•å®ç°ä¸ºç›´æ¥ç›¸åŠ åè¾“å‡º
#         out = q + attn_out
        
#         # [B, L, C] -> [B, C, L] -> [B, C, H, W]
#         out = out.permute(0, 2, 1).view(B, C_l, H_l, W_l)
        
#         return self.proj(out)
    







class GL_ContextBlock(nn.Module):
    def __init__(self, c_local, c_global, c_out=None, nhead=4, dropout=0.0):
        """
        ä¼˜åŒ–åçš„å…¨å±€-å±€éƒ¨ä¸Šä¸‹æ–‡çº ç¼ æ¨¡å— (Global-Local Context Entanglement)
        
        Args:
            c_local (int): ç»†è§†ç‰¹å¾é€šé“æ•° (Query)
            c_global (int): ç²—è§†ç‰¹å¾é€šé“æ•° (Key/Value)
            c_out (int, optional): è¾“å‡ºé€šé“æ•°. é»˜è®¤ä¸º Noneï¼Œå³ç­‰äº c_local
            nhead (int): å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            dropout (float): Dropout æ¯”ç‡
        """
        super().__init__()
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºé€šé“ï¼Œé»˜è®¤ä¿æŒä¸ local ä¸€è‡´
        if c_out is None:
            c_out = c_local
            
        self.c_local = c_local
        
        # 1. ç‰¹å¾å¯¹é½æŠ•å½±: å°† Global ç‰¹å¾æ˜ å°„åˆ°ä¸ Local ç›¸åŒçš„ç»´åº¦
        # è¿™æœ‰åŠ©äºåœ¨ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´è®¡ç®—ç›¸ä¼¼åº¦
        self.proj_global_k = nn.Conv2d(c_global, c_local, 1)
        self.proj_global_v = nn.Conv2d(c_global, c_local, 1)
        
        # 2. LayerNorm (Pre-Norm ç»“æ„)
        self.norm_l = nn.LayerNorm(c_local)
        self.norm_g = nn.LayerNorm(c_local)
        self.norm_ffn = nn.LayerNorm(c_local)

        # 3. Cross Attention
        # ä¼˜åŒ–: æ­¤æ—¶ kdim, vdim å‡å·²å¯¹é½ä¸º c_local
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=c_local, 
            num_heads=nhead, 
            dropout=dropout, 
            batch_first=True
        )
        
        # 4. FFN (Feed-Forward Network) - å¢å¼ºéçº¿æ€§è¡¨è¾¾
        # ä½¿ç”¨ ConvFFN (Conv1x1 -> DWConv3x3 -> GELU -> Conv1x1) 
        # ç›¸æ¯”æ™®é€š MLPï¼ŒDWConv èƒ½æ›´å¥½åœ°æå–å±€éƒ¨ç‰¹å¾ï¼Œé˜²æ­¢ä½ç½®ä¿¡æ¯ä¸¢å¤±
        hidden_dim = c_local * 4
        self.ffn = nn.Sequential(
            nn.Conv2d(c_local, hidden_dim, 1),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, groups=hidden_dim), # Depthwise
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Conv2d(hidden_dim, c_local, 1),
            nn.Dropout(dropout)
        )

        # 5. è¾“å‡ºæŠ•å½±
        self.proj_out = nn.Conv2d(c_local, c_out, 1) if c_local != c_out else nn.Identity()

    def _get_abs_pos_encoding(self, x):
        """
        åŠ¨æ€ç”Ÿæˆ 2D æ­£å¼¦ä½ç½®ç¼–ç  (Sinusoidal Positional Encoding)
        x: [B, C, H, W]
        Return: [1, L, C] (L=H*W)
        """
        B, C, H, W = x.shape
        # ç®€å•çš„å®ç°ï¼šç”Ÿæˆå½’ä¸€åŒ–çš„ç½‘æ ¼åæ ‡
        # æ³¨æ„ï¼šä¸ºäº†æ•ˆç‡ï¼Œå®é™…éƒ¨ç½²æ—¶å¯ä»¥ç¼“å­˜ï¼Œä½†åŠ¨æ€ç”Ÿæˆå¯¹å¤šå°ºåº¦è®­ç»ƒæ›´é²æ£’
        device = x.device
        
        y_embed = torch.arange(1, H + 1, dtype=torch.float32, device=device).unsqueeze(1).repeat(1, W).view(-1)
        x_embed = torch.arange(1, W + 1, dtype=torch.float32, device=device).repeat(H, 1).view(-1)
        
        # å°†åæ ‡å½’ä¸€åŒ–å¹¶ç¼©æ”¾ï¼Œæ¨¡æ‹Ÿé¢‘ç‡
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆçš„ PEï¼Œå°† x, y åæ ‡ç›´æ¥ä½œä¸ºè¾…åŠ©ç‰¹å¾åŠ è¿›å»
        # å¦‚æœè¿½æ±‚æè‡´ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„ sin/cos å…¬å¼ï¼Œä½†è¿™é€šå¸¸è¶³å¤Ÿäº†
        
        # ä¸ºäº†ä¸ç ´åç»´åº¦ï¼Œæˆ‘ä»¬ç®€å•åœ°ç”¨ sin/cos å¤„ç†ä¸€ä¸‹åæ ‡
        div_term = torch.exp(torch.arange(0, C, 2, dtype=torch.float32, device=device) * -(math.log(10000.0) / C))
        
        # [H*W, 1] * [C/2] -> [H*W, C/2]
        pe_x = torch.zeros(H * W, C, device=device)
        pe_y = torch.zeros(H * W, C, device=device)
        
        pe_x[:, 0::2] = torch.sin(x_embed.unsqueeze(1) * div_term)
        pe_x[:, 1::2] = torch.cos(x_embed.unsqueeze(1) * div_term)
        
        pe_y[:, 0::2] = torch.sin(y_embed.unsqueeze(1) * div_term)
        pe_y[:, 1::2] = torch.cos(y_embed.unsqueeze(1) * div_term)
        
        # èåˆ X å’Œ Y çš„ä½ç½®ä¿¡æ¯ (ç®€å•å¹³å‡)
        pe = (pe_x + pe_y) / 2.0
        return pe.unsqueeze(0) # [1, L, C]

    def forward(self, x_local, x_global):
        # [æ·»åŠ è¿™ä¸€è¡Œ]
        if not hasattr(self, 'traced'): # åªæ‰“å°ä¸€æ¬¡ï¼Œé˜²æ­¢åˆ·å±
            print(f"\nâœ… [éªŒè¯æˆåŠŸ] GL_ContextBlock æ­£åœ¨è¿è¡Œ! è¾“å…¥å°ºå¯¸: Local={x_local.shape}, Global={x_global.shape}")
            self.traced = True
        """
        x_local:  [B, C_l, H_l, W_l] (Fine, Query)
        x_global: [B, C_g, H_g, W_g] (Coarse, Key/Value)
        """
        B, C_l, H_l, W_l = x_local.shape
        B, C_g, H_g, W_g = x_global.shape

        # --- 1. é¢„å¤„ç† Global ç‰¹å¾ ---
        # æŠ•å½± Key å’Œ Value åˆ° Local ç»´åº¦ï¼Œæ–¹ä¾¿è®¡ç®—
        k_src = self.proj_global_k(x_global) # [B, C_l, H_g, W_g]
        v_src = self.proj_global_v(x_global) # [B, C_l, H_g, W_g]

        # --- 2. å±•å¹³ (Flatten) å¹¶æ·»åŠ ä½ç½®ç¼–ç  ---
        # Query: Local
        q = x_local.flatten(2).permute(0, 2, 1) # [B, L_l, C]
        # æ·»åŠ  PE ç»™ Query (å¯é€‰ï¼Œä½†æ¨è)
        q_pe = self._get_abs_pos_encoding(x_local)
        
        # Key/Value: Global
        k = k_src.flatten(2).permute(0, 2, 1)   # [B, L_g, C]
        v = v_src.flatten(2).permute(0, 2, 1)   # [B, L_g, C]
        # æ·»åŠ  PE ç»™ Key (éå¸¸å…³é”®ï¼è®© Query çŸ¥é“ Global ç‰¹å¾åœ¨å“ªé‡Œ)
        k_pe = self._get_abs_pos_encoding(k_src)

        # --- 3. Attention Block (Pre-Norm) ---
        # Norm -> Attn -> Add
        q_norm = self.norm_l(q)
        k_norm = self.norm_g(k)
        
        # æ³¨æ„: ä¼ å…¥ attn çš„ query å’Œ key åŠ ä¸Šä½ç½®ç¼–ç ï¼Œvalue ä¸åŠ 
        # è¿™æ˜¯ä¸€ç§å¸¸è§çš„ Transformer ä¼˜åŒ– (å¦‚ DETR)
        attn_out, _ = self.cross_attn(
            query = q_norm + q_pe, 
            key   = k_norm + k_pe, 
            value = v
        )
        
        # æ®‹å·®è¿æ¥ 1
        x = q + attn_out # [B, L_l, C]

        # --- 4. FFN Block (Pre-Norm) ---
        # éœ€è¦å…ˆ reshape å› 2D è¿›è¡Œå·ç§¯ FFN
        x_2d = x.permute(0, 2, 1).view(B, C_l, H_l, W_l)
        
        # FFN: Norm -> ConvFFN -> Add
        # è¿™é‡Œä¸ºäº†é…åˆ LayerNormï¼Œå…ˆ flatten å† norm å† reshapeï¼Œæˆ–è€…ç›´æ¥ç”¨ GroupNorm
        # ä¸ºä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å¤„ç† LayerNorm
        x_norm = self.norm_ffn(x).permute(0, 2, 1).view(B, C_l, H_l, W_l)
        
        ffn_out = self.ffn(x_norm)
        
        # æ®‹å·®è¿æ¥ 2
        out = x_2d + ffn_out # [B, C_l, H_l, W_l]

        # --- 5. æœ€ç»ˆè¾“å‡º ---
        return self.proj_out(out)